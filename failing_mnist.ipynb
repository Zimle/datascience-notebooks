{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What's this notebook about\n\nI started tackling the mnist problem with the following ambitions: I was going to try the following sklearn classifiers on the mnist data set:\n\n- KNeighborsClassifier\n- SGDClassifier\n- SVC with different kernels\n- RandomForest\n- ExtraTreesClassifier\n- GradientBoost\n- VotingClassifier\n\nTo reduce computing time or get better accuracy, I tried:\n\n- cropping the images\n- stratifying the images, meaning putting every pixel above a threshold 1 and all other zero\n- polluting the images with some random pixels\n- resize the images\n- augmente the training images with shifted and rotated versions\n\nI am a very beginner and working through the wonderfully written book \"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems\" by Aurelien Geron. Somehow, I spent a lot effort working on mnist ending up not being better than vanilla sklearn classifiers such as RandomForestClassifier or KNearestNeighbor. In this notebook, I will sketch my older notebooks commenting what I think went wrong. In retrospective, this notebook became a little lengthy, so sorry if you get bored. Comments on my initial notebook are set in *italics*. Feel free to tell me more on the mistakes I made, best practice I violate or maybe nonsense you made in yourself in your first projects.\n\nOne of the first problems I encountered was the following:"},{"metadata":{},"cell_type":"markdown","source":"## Failure: Don't use an old notebook"},{"metadata":{},"cell_type":"markdown","source":"**Update**: *After uploading to Kaggle, I see that I have to run through all cells again, seeing that Kaggle works much faster than my notebook. If I had known this before... Note that in the comments or markup cells, the previous runtime of my notebook is often given and not the output of the actual cell you see*\n\n**Update 2**: I found the reason why the SGDClassifier and the Nystroem-LinearSVC worked so poor on the augmented dataset. The transformation of the pictures with pixel range [0,255] gave pictures with pixel in range [0,1]. I corrected this mistake by dividing all train pixels by 255. So do not be surprised.\n\n*Nothing is more annoying than having to wait for your notebook output or running into memory problems after a long notebook session. With mnist, I realized that my notebook (the system settings says Intel® Core™ i3-3217U CPU @ 1.80GHz × 4) was way too slow to use a standard gridsearch on KNearestNeighbor or SVCs. In particular, you never get to know how long it will take.\nAnother issue was that later when augmenting the data set I sometimes ran into memory failures. At that point, even relatively simpe algorithms did not work any more, and I could not help but restart the kernel. Thats in particular annoying when running long notebooks. Either you wait for 30 minutes or so for the kernel to be at the point where it breaked, or you have to search the cells to run which were needed at the point your are.*\n\n*Therefore, my approach was how to go ahead with manipulating the pictures to compute faster or get better accuracy. Because of the time and memory issue, I just collected every import in the first cell, function in the second and class in the third cell to avoid searching the module I imported a function from. As you can see, I experimented a lot, but feel free to skip the functions or class. They will appear again when they are needed*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport time\nimport random\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# import sklearn functions\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ParameterGrid\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder,LabelEncoder, LabelBinarizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\n\n# other stuff needed\nfrom skimage.transform import resize\nfrom scipy.stats import uniform, expon, randint\nfrom scipy.ndimage.interpolation import shift, rotate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the next two functions are taken from Aurelien Gerons book \"Hands-On Machine Learning \n# with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems\",\n# from the notebook https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb\ndef plot_digit(data, cmap='gray', size=28):\n    image = data.reshape(size, size)\n    plt.imshow(image, cmap = cmap,interpolation=\"nearest\")\n    #plt.axis(\"off\")\n\ndef plot_digits(instances, images_per_row=10, size=28,**options):\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) // images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = 'gray', **options) # cmap = mpl.cm.binary\n    plt.axis(\"off\")\n\ndef fit_time_est(estimator, X_train, y_train, prnt=True):\n    start = time.time()\n    estimator.fit(X_train, y_train)\n    end = time.time()\n    if prnt:\n        print(\"Time needed to fit the estimator:\", end - start)\n    return end - start\n\ndef fit_score_time(estimator, X_train, y_train, X_val, y_val, prnt=True, oob=False):\n    time_fit = fit_time_est(estimator, X_train, y_train, prnt=False)\n    if oob:\n        if prnt:\n            print(\"Time needed for fitting:\", time_fit)\n            print(\"Estimate of generalization error from oob (out of bag samples):\", estimator.oob_score_)\n        #return est, time_fit, est.oob_score_\n    else:    \n        start = time.time()\n        y_pred = estimator.predict(X_val)\n        acc_score = accuracy_score(y_pred,y_val)\n        end = time.time()\n        if prnt:\n            print(\"Time needed for fitting:\", time_fit)\n            print(\"Valuation score:\", acc_score, \" with\", end - start, \"seconds needed to evaluate.\")\n    \ndef crop_inst(X_inst, tr_im_size=[28,28], crop_left=3, crop_right=3, crop_top=3, crop_bottom=3):\n    \"\"\"crop_data a data instance into its image_size, crops the given pixels from the border\n    and transforms the new image back into a row array\"\"\"\n    X_im = X_inst.reshape(tr_im_size)\n    X_im = X_im[crop_top:-crop_bottom, crop_left:-crop_right] # do the cropping\n    return X_im.reshape((tr_im_size[0]-crop_top-crop_bottom)*(tr_im_size[1]-crop_left-crop_right))\n    \n\ndef crop_data(X_train, tr_im_size=[28,28], crop_left=3, crop_right=3, crop_top=3, crop_bottom=3):\n    \"\"\"crop_data turns every data instance into its image_size, crops the given pixels from the border\n    and transforms the new image back into a row array, returning thus a new data set\"\"\"\n    return np.apply_along_axis(crop_inst,1,X_train,tr_im_size=tr_im_size,crop_left=crop_left,\n                               crop_right=crop_right, crop_top=crop_top, crop_bottom=crop_bottom)\n\ndef instance_resizer(instance, orig_im_size=(28,28), new_im_size=(14,14), mode='reflect', anti_aliasing=True):\n    \"\"\"takes a vector, interprets it as a picture, resizes it and returns the new image as a vector\"\"\"\n    image_orig = instance.reshape(orig_im_size)\n    image_new = resize(image_orig, new_im_size, mode=mode)\n    inst_transformed = image_new.reshape(new_im_size[0]*new_im_size[1])\n    return inst_transformed\n\ndef resize_train(X_train, orig_im_size=(28,28), new_im_size=(14,14), mode='reflect',anti_aliasing=True):\n    \"\"\"applies instance_resizer to every instance\"\"\"\n    return np.apply_along_axis(instance_resizer,1,X_train,orig_im_size=orig_im_size,new_im_size=new_im_size,\n                               mode=mode, anti_aliasing=anti_aliasing)\n\ndef stratify_data(X_train, threshold=230):\n    return np.where(X_train>threshold/255, 1, 0).astype(np.bool) # the /255 comes from a former version where the pixels where not normalized\n\ndef pollute_instance(instance, im_size=(28,28), n_poll_pic=1, random_state = None):\n    if not random_state is None:\n        np.random.seed(random_state)\n    inst = instance.copy() # otherwise, we change the original data\n    image = inst.reshape(im_size)\n    # set rectangle with pollution\n    for _ in range(n_poll_pic):\n        a, b = np.random.randint(0,im_size[0]), 1+int(4*np.random.uniform())\n        c, d = np.random.randint(0,im_size[1]), 1+int(4*np.random.uniform())\n        # start polluting\n        for i in range(a,a+b):\n            for j in range(c,c+d):\n                try:\n                    image[i,j] = np.random.randint(50,200)/255 # /255 is added because pixels are assumed to be in [0,1]\n                except IndexError:\n                    pass\n    inst_transformed = image.reshape(im_size[0]*im_size[1])\n\n    return inst_transformed\n    \n\ndef pollute_training(X_train, im_size=(28,28), n_poll_pic=1, random_state = None):\n    # usually, we would have used the following code, but it does not work with random_state set because in each\n    # instance the same pixels are polluted\n    if random_state is None:\n        return np.apply_along_axis(pollute_instance,1,X_train,im_size=im_size,\n                                   n_poll_pic=n_poll_pic, random_state=random_state)\n    else:\n        X_pol = X_train.copy()\n        for i in range(X_train.shape[0]):\n            X_pol[i] = pollute_instance(X_train[i],im_size=im_size, n_poll_pic=n_poll_pic,\n                                        random_state=random_state+i)\n        return X_pol\n\n# shift_instance and rand_shift_training are adapted versions from Aurelien Gerons book chapter 3 additional material\n# see his notebooks\n# to avoid combinatorial explosion, we use random shifting and rotating\n\ndef shift_instance(instance, dx, dy, new=0, im_size=(28,28)):\n    return shift(instance.reshape(im_size), [dy, dx], cval=new).reshape(im_size[0]*im_size[1])\n\ndef rand_shift_training(X_train, y_train, dx_max=2, dy_max=2, n_shifts = 4, new=0,\n                        im_size=(28,28), random_state=None, only_axes=True, do_shuffle=True):\n    \n    if not random_state is None:\n        np.random.seed(random_state)\n    \n    # collect all possibilities to move an image with given dx_max, dy_max\n    moving_vectors = []\n    if only_axes:\n        for i in range(1,dx_max+1): # if i=0 or j=0, we will get double entries. we will remove them below\n            moving_vectors.append((i,0))\n            moving_vectors.append((-i,0))\n        for i in range(1,dy_max+1):\n            moving_vectors.append((0,i))\n            moving_vectors.append((0,-i))\n    else:\n        for i in range(0,dx_max+1):\n            for j in range(0,dy_max+1): # if i=0 or j=0, we will get double entries. we will remove them below\n                moving_vectors.append((i,j))\n                moving_vectors.append((-i,j))\n                moving_vectors.append((i,-j))\n                moving_vectors.append((-i,-j))\n    \n    moving_vectors = list(set(moving_vectors)) # removes double entries from moving_vectors\n\n    # we do not want to double images, so remove (0,0) from moving_vectors\n    try:\n        moving_vectors.remove((0,0))\n    except ValueError:\n        if not only_axes: # because there should be no (0,0) inside if only_axes is True\n            print(\"No (0,0) could be removed from moving_vectors. Please check the code.\")\n    \n    \n    X_train_expanded = []\n    y_train_expanded = []\n    \n    # move image by moving_vectors and add to X_train_expanded\n    for i in range(X_train.shape[0]):\n        try:\n            for dx, dy in random.sample(moving_vectors, n_shifts):\n                shifted_image = shift_instance(X_train[i], dx=dx, dy=dy, new=0, im_size=im_size)\n                X_train_expanded.append(shifted_image)\n                y_train_expanded.append(y_train[i])\n        except ValueError:\n            print(\"Using maximal possible shifts. This might be smaller thatn n_shifts.\")\n            for dx, dy in moving_vectors:\n                shifted_image = shift_instance(X_train[i], dx=dx, dy=dy, new=0, im_size=im_size)\n                X_train_expanded.append(shifted_image)\n                y_train_expanded.append(y_train[i])\n\n    X_train_expanded = np.array(X_train_expanded, dtype=np.uint8)\n    y_train_expanded = np.array(y_train_expanded, dtype=np.uint8)\n    \n    # shuffle the new shifted images to avoid classifiers see the same number for too many times\n    if do_shuffle:\n        X_train_expanded, y_train_expanded = shuffle(X_train_expanded, y_train_expanded, random_state=random_state)\n    return X_train_expanded, y_train_expanded\n\ndef rand_rotate_instance(instance, angle=10, new=0, im_size=(28,28)):\n    return rotate(instance.reshape(im_size), angle=angle, cval=new, reshape=False).reshape(im_size[0]*im_size[1])\n\ndef rand_rotate_training(X_train, y_train, angle_range=(5,10), im_size=(28,28),\n                        sign=True, do_shuffle=True, random_state=None):\n    \n    if not random_state is None:\n        np.random.seed(random_state)\n    \n    X_rot = X_train.copy()\n    for i in range(X_train.shape[0]): # we need this loop to not always get the same rotation\n        angle = np.random.randint(angle_range[0],angle_range[1])\n        X_rot[i] = rand_rotate_instance(X_train[i],angle=angle,im_size=im_size)\n    \n    if sign:\n        X_rot_sign = X_train.copy()\n        for i in range(X_train.shape[0]): # we need this loop to not always get the same rotation\n            angle = np.random.randint(angle_range[0],angle_range[1])\n            X_rot_sign[i] = rand_rotate_instance(X_train[i],angle=-angle,im_size=im_size)\n        \n        # shuffle the new shifted images to avoid classifiers see the same number for too many times\n        if do_shuffle:\n            X_train_expanded, y_train_expanded = shuffle(np.concatenate((X_rot, X_rot_sign)),\n                                                         np.concatenate((y_train, y_train)),\n                                                         random_state=random_state)\n        return X_train_expanded, y_train_expanded    \n    else:\n        return X_rot, y_train\n    \ndef augment_training(X_train, y_train,dx_max=2,dy_max=2,n_shifts = 4,angle_range=(5,10),im_size=(28,28),\n                     only_axes=False, do_shuffle=True, sign=True, random_state=None):\n    X_shift, y_shift = rand_shift_training(X_train, y_train, dx_max=dx_max, dy_max=dy_max, n_shifts = n_shifts, \n                        im_size=im_size, random_state=random_state, only_axes=only_axes, do_shuffle=do_shuffle)\n    X_rot, y_rot = rand_rotate_training(X_train, y_train, angle_range=angle_range, im_size=im_size,\n                        sign=sign, do_shuffle=do_shuffle, random_state=random_state)\n    X_train = np.concatenate((X_train,X_shift, X_rot))\n    y_train = np.concatenate((y_train, y_shift, y_rot))\n    return X_train, y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageTransformer(BaseEstimator, TransformerMixin): # the arguments say that the new class inherits from the others, namely get/setparams_and fit_transform\n    def __init__(self, resize=True, pollute=True, crop=True, stratify=True, n_poll_pic=1, random_state=None): # pass only whether to apply some transformations and random state\n        self.resize = resize\n        self.crop = crop\n        self.stratify = stratify\n        self.pollute = pollute\n        self.n_poll_pic = n_poll_pic\n        self.random_state = random_state\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X): # the X should be an array accessible by dataframe.values. Important as pipeline only handles these guys\n        if self.pollute:\n            X = pollute_training(X,n_poll_pic = self.n_poll_pic, random_state=self.random_state)\n        if self.resize:\n            if X.shape[1] != 14*14: # only resize if pictures are not already resized to (14,14)\n                X = resize_train(X)\n        if self.crop:\n            X = crop_data(X, tr_im_size=[14,14], crop_left=1, crop_right=1, crop_top=1, crop_bottom=1)\n        if self.stratify:\n            stratify_data(X, threshold=0)\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Having most of the important things I had done so far at the very beginning was practical as you always know where to scroll or which cells to run if restarting the notebook. But most notebooks I have seen do this on the fly, so I guess I am violating best practices here.*\n\n*The following was the outset of a tutorial I had in mind (haha!). So maybe with regard to the many classifiers I wanted to use, I should record:*"},{"metadata":{},"cell_type":"markdown","source":"## Failure : Don't make big plans as a beginner"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\ndf_train = pd.read_csv(\"../input/digit-recognizer/train.csv\", engine='c')\ndf_test = pd.read_csv(\"../input/digit-recognizer/test.csv\", engine='c')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect the data\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for completeness\n# only labels in the range of 0..9 ???\ndf_train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so the labels appear more or less in the same frequence. in particular, accuracy score is fine\ndf_train['label'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for null values\ndf_train.isnull().any().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no null value. great. lets start by taking the numpy arrays and splitting the label from the instances\ny = df_train['label'].to_numpy(dtype = np.uint8)\nX = df_train.drop(labels = [\"label\"],axis = 1).to_numpy()/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check sizes\nprint(y.shape)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data\nWe split our data up into the following parts:\n\n- 32000 instances of: X_red_train for training models\n- 5000 instances of : X_val for evaluating our estimators when we do not use cross-validation due to time consuming training\n- 5000 instances of : X_test to look for the generalization error we are going to make, as by looking for parameters, we might overfit the validation data and checking on test data might bring us back to earth\n\nThe final estimator will be trained on the full data set and then applied to the test data of kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=5000, random_state=43, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=5000, random_state=43, stratify=y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are working on a classification problem. The numbers 0,...,9 can be interpreted by the algorithm as being near, meaning that e.g 0 and 1 are more related than 0 and 9. For those algorithms capable of interpreting, we use sklearns LabelBinarizer. The OneHotEncoder is meant for use on the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_bin = LabelBinarizer()\ny_train_label = lab_bin.fit_transform(y_train)\ny_val_label = lab_bin.transform(y_val) # dont fit again, so that the correspondance of digit and list keeps the same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets have a look how it works:\nprint(y_train[:10])\ny_train_label[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*This is the point where I always used \"run all cells above\" when I restarted a kernel, because it defined the training data. Until now, I don't feel of having made any mistakes, but you are welcome to point some out.*\n\n*The next part was about looking at the data, where I present the main ideas of manipulating the pictures:*"},{"metadata":{},"cell_type":"markdown","source":"## Inspecting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see pictures\nplt.figure(figsize=(9,9))\nexample_images = X_train[:100]\nplot_digits(example_images, images_per_row=10)\n#save_fig(\"more_digits_plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_digit(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we might crop some pictures to reduce dimensions, so from the left and right we could remove 3 pixels and from the top and bottom 3 pixels (actually, we could remove even more from the top most often). Note how the data seems to be cropped by default like the first six. Some ideas\n\n- use the cropping as a feature, maybe centering the images horizontally and vertically. Note that centering makes us unable to use data augmentation by moving the image along axes, leaving only rotation\n- remove outlier pixels as in the 6 shown in the middle of the hundred digits by removing pixels where the sum of neighbor pixels is low\n- make the gray-scale binary by saying that everything above a threshold (say 230) is set to 255\n- the drawing tends to be thick enough so that we can scale the pictures to 14x14 pictures without loosing to much information. the interpolation used is a hyperparameter\n\nFurthermore, we will classically augment the training set by some shifts and rotations."},{"metadata":{},"cell_type":"markdown","source":"*I still think this sounds reasonable, but it turns out that the main good technique that worked was the resizing. The next section was considered to give a bar which is to raise: a simple Random Forest*"},{"metadata":{},"cell_type":"markdown","source":"## A first shot: RandomForest\n\nAs a first attempt, we will just apply a vanilla RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the performance of a random forest as a bar\n# they are rather fast, so we can apply the whole training set\nrdf_clf = RandomForestClassifier(random_state=43, oob_score=True, n_jobs=-1) # n_jobs=-1 takes all processors\n\nfit_score_time(rdf_clf, X_train, y_train, X_val, y_val, prnt=True, oob=True) # took 23 seconds, score 0.958","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given that we did not anything special, this is a quick and good solution. At least we now know that the difficulty is to get a very good score. But wait: Shouldn't we apply categorical labeling? Lets give it a try:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_score_time(rdf_clf, X_train, y_train_label, X_val, y_val, prnt=True, oob=True) # took 45 seconds,score 0.986","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note that for a long time I thought that using onehotencoding to the labels would increase the score. Later on I found out that the score on the validation set is much lower, so I thought of overfitting. But training with y_train makes out-of-bag score (oob_score) more or less equal to the score on the validation set. I still cannot explain this issue, so I would be very happy if you could shed some light on this issue.*\n\nThat's way better! But be warned that we did not restrict the depth of the trees and so on, and we will see that the oob score is much higher than on the validation set. And note that we only trained on 32000 instances. Another reason why RandomForests are often a great first shot is that they can tell you which feature they feel are important:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see, a great part of the picture is unimportant as we already saw\nplot_digit(rdf_clf.feature_importances_, cmap=mpl.cm.hot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The things done so far were more or less okay, I think. In the following section, you see how I chose the wrong track: I made lots of calculations which are of no value: I trained several models on a way too small dataset. Note that I first tried the whole training set or big parts of it, which ended in interrupting (and sometimes crashing) the kernel. So be warned that you only see the tip of the iceberg here:*\n\n## Looking how our proposed vanilla models work\n\nWe take a look at the models proposed at the beginning training them on small datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# WARNING: this code took about 150 seconds on my machine\nknn = KNeighborsClassifier()\nsgd = make_pipeline(StandardScaler(),SGDClassifier(random_state=43))\nx_trees = ExtraTreesClassifier(random_state=43)\nsvc_rbf = make_pipeline(StandardScaler(),SVC(kernel='rbf', random_state=43)) #  kernel='rbf' is the default in sklearn\nlin_svc = make_pipeline(StandardScaler(),LinearSVC(random_state=43))\ngrad_boost = GradientBoostingClassifier(random_state=43)\n\nclassifiers = [knn, sgd,x_trees,svc_rbf,lin_svc, grad_boost]\n\nstart = time.time()\nfor clf in classifiers:\n    print(clf.__class__.__name__)\n    for i in [100,300,900]:\n        fit_score_time(clf, X_train[:i], y_train[:i], X_val, y_val, prnt=True, oob=False)\n    print(\"**************************************************************\")\nend = time.time()\nprint(\"Total time needed:\", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GradientBoostingClassifier already took long on 1000 instances. Lets check the others additionally on 2700 instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"# WARNING: This code took about 74 seconds on my machine\nknn = KNeighborsClassifier()\nsgd = make_pipeline(StandardScaler(),SGDClassifier(random_state=43)) # SGD needs all features to have a similar scale (which we have), so we do not try scaling yet\nx_trees = ExtraTreesClassifier(random_state=43)\nsvc_rbf = make_pipeline(StandardScaler(),SVC(kernel='rbf', random_state=43)) # kernel='rbf' is the default\nlin_svc = make_pipeline(StandardScaler(),LinearSVC(random_state=43))\ngrad_boost = GradientBoostingClassifier(random_state=43)\n\nclassifiers = [knn, sgd,x_trees,svc_rbf,lin_svc]\n\nstart = time.time()\nfor clf in classifiers:\n    print(clf.__class__.__name__)\n    for i in [2700]:\n        fit_score_time(clf, X_train[:i], y_train[:i], X_val, y_val, prnt=True, oob=False)\n    print(\"**************************************************************\")\nend = time.time()\nprint(\"Total time needed:\", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*So these were about 3.5 minutes computation, and more time to compare the times and scores manually. But I will not use them again, and they do not generalize well. I do not think that the 2700 instance training was sensible, and at a former version, I already started some hyperparameter training on training sizes of this range at this stage, which turned out useless and very time consuming. I wanted to see if I could get some good scores. So*\n\n## Failure: Don't be impatient and greedy"},{"metadata":{},"cell_type":"markdown","source":"*The function \"fit_score_time\" was written to print something nicely out, but already at this stage passing the times and scores to a DataFrame would be much more lucid. And it would not \"spam\" the notebook. Be aware that this will become more and more advisable later, but at the point I realized that I should have done it, I was too lazy to adjust the changes in all the code.*\n\n## Failure:  Design your functions properly / don't sort data by hand"},{"metadata":{},"cell_type":"markdown","source":"*Note that at this point I already knew that I did not want to include LinearSVC into a VotingClassifier, as there  would have been an SVC with kernel rbf or another solution (I found out that LinearSVC with Nystroem kernel approximation can be a good substitution, which I later included. Do not ask me why I still kept LinearSVC in the list below.*"},{"metadata":{},"cell_type":"markdown","source":"So here are some observations:\n\nExcept for LinearSVC, all of the classifiers seem to work and become better. Lets focus on the training and validation time:\n\nKNeighborsClassifier: needs very few time to fit no matter what the training size is, but tends to grow more than linear. the validation seems to grow linear with the number of training instances, so training on 32000 instances would take probably 416 seconds on my machine. That's no fun when doing cross validation.\n\nExtraTreesClassifier should behave like random forest, so there are no time issues here to discuss (at least on my machine). note that we should have applied the y_train_label to get better scores.\n\nSGDClassifier: behaves in training and validation time like x_trees, so it should be no problem to apply it to the whole training data. note that it did not profit from training 900 or 2700 instances\n\nlin_svc: seems to reach its top soon and will probably not make it into 95% and above, and it takes too much time\n\nsvc_rbf: seems to get much higher, but its training time grows more than linearly and - not to underestimate - its predicting time roughly grows linearly with the number of training instances: Training on 35000 instances will likely take 187 seconds predicting on the validation set on my machine, so too long for making much crossvalidation (apart from the fitting time that will raise non-linear)\n\ngrad_boost: Seems to work fine, but we have to look whether we could make it work in less time.\n\nNext, just try manipulating our digit pictures!"},{"metadata":{},"cell_type":"markdown","source":"*In the next chapter, I discuss the cropping of picture, which I show you as an example how all the next sections were build up.*"},{"metadata":{},"cell_type":"markdown","source":"## Cropping images\n\nIn this section, we are going to cut the borders of the image and look for the performance of our classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the functions that will do the cropping\ndef crop_inst(X_inst, tr_im_size=[28,28], crop_left=3, crop_right=3, crop_top=3, crop_bottom=3):\n    \"\"\"crop_data a data instance into its image_size, crops the given pixels from the border\n    and transforms the new image back into a row array\"\"\"\n    X_im = X_inst.reshape(tr_im_size)\n    X_im = X_im[crop_top:-crop_bottom, crop_left:-crop_right] # do the cropping\n    return X_im.reshape((tr_im_size[0]-crop_top-crop_bottom)*(tr_im_size[1]-crop_left-crop_right))\n    \n\ndef crop_data(X_train, tr_im_size=[28,28], crop_left=3, crop_right=3, crop_top=3, crop_bottom=3):\n    \"\"\"crop_data turns every data instance into its image_size, crops the given pixels from the border\n    and transforms the new image back into a row array, returning thus a new data set\"\"\"\n    return np.apply_along_axis(crop_inst,1,X_train,tr_im_size=tr_im_size,crop_left=crop_left,\n                               crop_right=crop_right, crop_top=crop_top, crop_bottom=crop_bottom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# crop_inst should work:\nplot_digit(crop_inst(X_train[43]), size=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# crop_data should work and not take too long\nstart = time.time()\nX_crp = crop_data(X_train)\nend = time.time()\nprint(\"Time needed to crop all instances:\", end-start)\nplot_digit(X_crp[43], size=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see how a random forest performs on the cropped set\nrdf_clf = RandomForestClassifier(random_state=43, oob_score=True, n_jobs=-1)\n\nfit_score_time(rdf_clf, X_crp, y_train_label, crop_data(X_val), y_val_label, prnt=True, oob=True) # took 50 seconds, score 0.987","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey, thats 0.9867 vs 0.9864. But we wanted to reduce the time, and both roughly took 50 seconds even if we dropped 300 pixels per image. I guess that the RandomForest was intelligent enough to see this. But now take a look at the other classifiers and how they perform:"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nfor clf in classifiers: # reminder: classifiers = [knn,sgd,x_trees,svc_rbf,lin_svc]\n    print(clf.__class__.__name__)\n    fit_score_time(clf, X_crp[:2700], y_train[:2700], crop_data(X_val), y_val, prnt=True, oob=False)\n    print(\"**************************************************************\")\n    \nend = time.time()\nprint(\"Total time needed:\", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here are the approximate times for 2700 training instances needed:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|not cropped   |  36   |      2.2 |     2.5  |   22.5   | 11.2    |\n|cropped       |  22.5 |      1.6 |     2.8  |   13.9   | 7.5     |\n\nAlthough it is not an incredible boost in computing time, it is either not bad. Now the scores:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|not cropped   | 0.916 |    0.842 |   0.933  |   0.904  | 0.805   |\n|cropped       | 0.913 |    0.826 |   0.930  |   0.925  | 0.803   |\n\n*See, I started manually writing tables with the results...*\n\nWhile the svc_rbf performed better and the sgd worse, I would't pay too much attention on the score difference as we did not trained several times on different training data. Lets observe that we do not loose to much information. Note that cropping might be bad to reach a top score as the difficulty lies in classifying the odd pictures. But for performance, this might be enough. Lets test cropping on the whole dataset where the classifier is quick enough (although still taking 235 seconds on my machine):\n\n*As I said, I was not patient enough to wait for results on all training instances. From above, it is clear that this will take some minutes.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# takes 235 seconds on my machine\nif False: # to prevent the kernel to compute again\n    start = time.time()\n    print(\"Training instances are not cropped:\")\n    for clf in [sgd,x_trees]:\n        print(clf.__class__.__name__)\n        fit_score_time(clf, X_train, y_train, X_val, y_val, prnt=True, oob=False)\n        print(\"**************************************************************\")\n\n    print(\"Training instances are cropped:\")\n    for clf in [sgd,x_trees]:\n        print(clf.__class__.__name__)\n        fit_score_time(clf, X_crp, y_train, crop_data(X_val), y_val, prnt=True, oob=False)\n        print(\"**************************************************************\")\n    end = time.time()\n    print(\"Total time needed:\", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey, at least those too perform slightly better. At least, it is unlikely that the scoring will get much too worse."},{"metadata":{},"cell_type":"markdown","source":"*The next sections are only introduced with their pictures and the markdown cells, as the code remains similar. Only the training on the full training set is reduced to SGDClassifier and ExtraTreesClassifier*"},{"metadata":{},"cell_type":"markdown","source":"## Stratifying images\n\nWe just want pictures where all pixels are either 1 or 0. Lets see how it performs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the function is really easy and should be very fast\ndef stratify_data(X_train, threshold=230):\n    return np.where(X_train>threshold/255, 1, 0).astype(np.bool)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Okay, lets apply\nX_strat = stratify_data(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no error yet, so lets look if it works:\nplt.figure(figsize=(9,9))\nplot_digits(X_strat[:100], images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*I ended up seeing that the threshold 230 was too high, and tested it with 130 (so that's meant when stratified is said). In fact, setting the threshold to 0 (that means every pixel with value bigger than zero is set to 1) yielded better results. Here are some pictures:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# threshold set to 130\nplt.figure(figsize=(9,9))\nplot_digits(stratify_data(X_train, threshold=130)[:100], images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# threshold set to 0\nplt.figure(figsize=(9,9))\nplot_digits(stratify_data(X_train, threshold=0)[:100], images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we collect the data: Time needed (I assume that the stratifying is so quick that fitting the validation data does not count too much)\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    |  36   |      2.2 |     2.5  |   22.5   | 11.2    |\n|cropped       |  22.5 |      1.6 |     2.8  |   13.9   | 7.5     |\n|stratified    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   |\n|strat extreme | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n\nAnd the scores:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    | 0.916 |    0.842 |   0.933  |   0.904  | 0.805   |\n|cropped       | 0.913 |    0.826 |   0.930  |   0.925  | 0.803   |\n|stratified    | 0.895 |   0.831  | 0.921    |   0.901  |  0.766  |\n|strat extreme | 0.922 |   0.853  | 0.941    |   0.917  |  0.794  |\n\nFor threshold=130, we have a loss in our scoring without being quicker. Thats surprising in particular for the xtrees, because the random forest was much quicker. But setting threshold=0, the scores become a little bit better for all classifiers. Lets try SGD and xtrees on all of the training set.\n\n*After training SGD and xtrees, I gave stratify_data_soft a shot, meaning that each pixel value below threshold is turned to 0, the others remaining the same.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stratify_data_soft(X_train, threshold=130):\n    return np.where(X_train>threshold/255, X_train, 0) # /255 added because data pixels are assumed to be in [0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stratify_data_soft with threshold=130:\nplt.figure(figsize=(9,9))\nplot_digits(stratify_data_soft(X_train, threshold=130)[:100], images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we collect the data: Time needed (I assume that the stratifying is so quick that fitting the validation data does not count too much)\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    |  36   |      2.2 |     2.5  |   22.5   | 11.2    |\n|cropped       |  22.5 |      1.6 |     2.8  |   13.9   | 7.5     |\n|stratified    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   |\n|strat extreme | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n|soft strat    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n\nAnd the scores:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    | 0.916 |    0.842 |   0.933  |   0.904  | 0.805   |\n|cropped       | 0.913 |    0.826 |   0.930  |   0.925  | 0.803   |\n|stratified    | 0.895 |   0.831  | 0.921    |   0.901  |  0.766  |\n|strat extreme | 0.922 |   0.853  | 0.941    |   0.917  |  0.794  |\n|strat soft    | 0.892 |   0.818  | 0.918    |   0.899  |  0.769  |\n\nAs expected, the scoring is worse when doin soft stratifying."},{"metadata":{},"cell_type":"markdown","source":"## Resizing the images\n\nHere, we are going to rescale the images to reduce dimension and see how much information will be lost. First start playing around:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape the data to 28x28 image\nsome_digit = X_train[42]\nsome_image = some_digit.reshape(28,28)\n\n# resize the image to 7x7\nfrom skimage.transform import resize\nsome_scaled_image = resize(some_image,(7,7))\n\nplt.imshow(some_scaled_image, cmap = 'gray') # should be a 9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Well, this will certainly be too small to train any digit recognizer (but maybe, you can try!)\n# reshape the data to 28x28 image\nsome_digit = X_train[41]\nsome_image = some_digit.reshape(28,28)\n\n# resize the image to 7x7\nfrom skimage.transform import resize\nsome_scaled_image = resize(some_image,(14,14))\n\nplt.imshow(some_scaled_image, cmap = 'gray') # should be a nine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this seems reasonable or at least manageable. lets define the functions (note that the\n# mode of the resizer and anti_aliasing are hyperparameter as well. we won't dive in this\n\ndef instance_resizer(instance, orig_im_size=(28,28), new_im_size=(14,14), mode='reflect', anti_aliasing=True):\n    \"\"\"takes a vector, interprets it as a picture, resizes it and returns the new image as a vector\"\"\"\n    image_orig = instance.reshape(orig_im_size)\n    image_new = resize(image_orig, new_im_size, mode=mode)\n    inst_transformed = image_new.reshape(new_im_size[0]*new_im_size[1])\n    return inst_transformed\n\ndef resize_train(X_train, orig_im_size=(28,28), new_im_size=(14,14), mode='reflect',anti_aliasing=True):\n    \"\"\"applies instance_resizer to every instance\"\"\"\n    return np.apply_along_axis(instance_resizer,1,X_train,orig_im_size=orig_im_size,new_im_size=new_im_size,\n                               mode=mode, anti_aliasing=anti_aliasing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply on some digit\nplt.imshow(instance_resizer(X_train[41]).reshape(14,14), cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks fine. does it work on X_train? how long will it take?\nstart = time.time()\nX_res = resize_train(X_train)\nend = time.time()\nprint(\"Time needed for resizing:\", end - start) # about 44 seconds\nplt.imshow(X_res[41].reshape(14,14), cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks nice. lets plot some of them:\nplt.figure(figsize=(9,9))\nplot_digits(X_res[:100], images_per_row=10, size=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we add the stats (I should have altered the functions and use a pd DataFrame...)\n\nTime needed\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    |  36   |      2.2 |     2.5  |   22.5   | 11.2    |\n|cropped       |  22.5 |      1.6 |     2.8  |   13.9   | 7.5     |\n|stratified    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   |\n|strat extreme | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n|soft strat    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n|resize        | 9.7   |   0.61   | 1.4      |   5.8    |  3.2    |\n\nAnd the scores:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    | 0.916 |    0.842 |   0.933  |   0.904  | 0.805   |\n|cropped       | 0.913 |    0.826 |   0.930  |   0.925  | 0.803   |\n|stratified    | 0.895 |   0.831  | 0.921    |   0.901  |  0.766  |\n|strat extreme | 0.922 |   0.853  | 0.941    |   0.917  |  0.794  |\n|strat soft    | 0.892 |   0.818  | 0.918    |   0.899  |  0.769  |\n|resize        | 0.930 |   0.882  | 0.937    |   0.921  |  0.859  |\n\nSo all in all, it not only gave us a time boost (okay, we killed 588 features from 784), but also increased the scoring. Lets check if this holds on the whole training set:"},{"metadata":{},"cell_type":"markdown","source":"## Polluting the pictures\n\nWith stratifying, we have focused our digits on the essential forms. As the objective is to label the hard pictures right, we may want to force the classifiers to focus on the essential parts of a number by polluting the picture with some random pixels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_digit(X_train[54])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# whats roughly the value for the pollution?\nprint(X_train[54].reshape(28,28)[5,22])\nprint(X_train[54].reshape(28,28)[4,22])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here are the functions:\ndef pollute_instance(instance, im_size=(28,28), n_poll_pic=1, random_state = None):\n    if not random_state is None:\n        np.random.seed(random_state)\n    inst = instance.copy() # otherwise, we change the original data\n    image = inst.reshape(im_size)\n    # set rectangle with pollution\n    for _ in range(n_poll_pic):\n        a, b = np.random.randint(0,im_size[0]), 1+int(4*np.random.uniform())\n        c, d = np.random.randint(0,im_size[1]), 1+int(4*np.random.uniform())\n        # start polluting\n        for i in range(a,a+b):\n            for j in range(c,c+d):\n                try:\n                    image[i,j] = np.random.randint(50,200)/255 # /255 added afterwards because pixels now assumed to be in [0,1]\n                except IndexError:\n                    pass\n    inst_transformed = image.reshape(im_size[0]*im_size[1])\n\n    return inst_transformed\n    \n\ndef pollute_training(X_train, im_size=(28,28), n_poll_pic=1, random_state = None):\n    # usually, we would have used the following code, but it does not work with random_state set because in each\n    # instance the same pixels are polluted\n    if random_state is None:\n        return np.apply_along_axis(pollute_instance,1,X_train,im_size=im_size,\n                                   n_poll_pic=n_poll_pic, random_state=random_state)\n    else:\n        X_pol = X_train.copy()\n        for i in range(X_train.shape[0]):\n            X_pol[i] = pollute_instance(X_train[i],im_size=im_size, n_poll_pic=n_poll_pic,\n                                        random_state=random_state+i)\n        return X_pol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets try:\nstart = time.time()\nX_pol = pollute_training(X_train, random_state=43)\nend = time.time()\nprint(\"Time needed for polluting training data:\", end - start) # is about 5 seconds, so quick enough\nplt.figure(figsize=(10,10))\nplot_digits(X_pol[:100], images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nice. lets test two polluted areas:\nplt.figure(figsize=(10,10))\nplot_digits(pollute_training(X_train[:100], n_poll_pic=2), images_per_row=10, size=28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the training time of the random forest did not change on the polluted data, whereas the score decreased slightly (0.004)\n\nAdd the stats\n\nTime needed\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    |  36   |      2.2 |     2.5  |   22.5   | 11.2    |\n|cropped       |  22.5 |      1.6 |     2.8  |   13.9   | 7.5     |\n|stratified    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   |\n|strat extreme | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n|soft strat    | 34.2  |   1.8    | 2.5      |   23.5   |  11.1   | times are roughly as stratified\n|resize        | 9.7   |   0.61   | 1.4      |   5.8    |  3.2    |\n|polluted      |  36   |      2.2 |     2.5  |   22.5   | 11.2    | times are roughly as unmodified\n\nAnd the scores:\n\n|training data |  knn  |     sgd  |  xtrees  | svc_rbf  | lin_svc |\n|--------------|-------|----------|----------|----------|---------|\n|unmodified    | 0.916 |    0.842 |   0.933  |   0.904  | 0.805   |\n|cropped       | 0.913 |    0.826 |   0.930  |   0.925  | 0.803   |\n|stratified    | 0.895 |   0.831  | 0.921    |   0.901  |  0.766  |\n|strat extreme | 0.922 |   0.853  | 0.941    |   0.917  |  0.794  |\n|strat soft    | 0.892 |   0.818  | 0.918    |   0.899  |  0.769  |\n|resize        | 0.930 |   0.882  | 0.937    |   0.921  |  0.859  |\n|polluted      | 0.914 |    0.856 |   0.933  |   0.922  | 0.821   |\n\nSo training took the same time on the polluted data, and accuracy did not become worse. In fact, the weaker learners svc's and sgd profited.\n\nLets test it on the whole resized dataset:"},{"metadata":{},"cell_type":"markdown","source":"Not bad: The SGDClassifier jumped from 88.7 to 91.6, and x_trees kept ist score. Maybe, we will later play more with polluting (in particular number of polluted areas. Lets turn to:"},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation\n\nThis is a standard technique and known to work well for MNIST. Although it will increase training time a lot, cropping and resizing might be so fast that my machine will not make too much trouble. *Haha, thats the point were I started to see memory failures*. Here we go:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift_instance and rand_shift_training are adapted versions from Aurelien Gerons book chapter 3 additional material\n# see his notebooks\n# to avoid combinatorial explosion, we use random shifting and rotating\n\ndef shift_instance(instance, dx, dy, new=0, im_size=(28,28)):\n    return shift(instance.reshape(im_size), [dy, dx], cval=new).reshape(im_size[0]*im_size[1])\n\ndef rand_shift_training(X_train, y_train, dx_max=2, dy_max=2, n_shifts = 4, new=0,\n                        im_size=(28,28), random_state=None, only_axes=True, do_shuffle=True):\n    \n    if not random_state is None:\n        np.random.seed(random_state)\n    \n    # collect all possibilities to move an image with given dx_max, dy_max\n    moving_vectors = []\n    if only_axes:\n        for i in range(1,dx_max+1): # if i=0 or j=0, we will get double entries. we will remove them below\n            moving_vectors.append((i,0))\n            moving_vectors.append((-i,0))\n        for i in range(1,dy_max+1):\n            moving_vectors.append((0,i))\n            moving_vectors.append((0,-i))\n    else:\n        for i in range(0,dx_max+1):\n            for j in range(0,dy_max+1): # if i=0 or j=0, we will get double entries. we will remove them below\n                moving_vectors.append((i,j))\n                moving_vectors.append((-i,j))\n                moving_vectors.append((i,-j))\n                moving_vectors.append((-i,-j))\n    \n    moving_vectors = list(set(moving_vectors)) # removes double entries from moving_vectors\n\n    # we do not want to double images, so remove (0,0) from moving_vectors\n    try:\n        moving_vectors.remove((0,0))\n    except ValueError:\n        if not only_axes: # because there should be no (0,0) inside if only_axes is True\n            print(\"No (0,0) could be removed from moving_vectors. Please check the code.\")\n    \n    \n    X_train_expanded = []\n    y_train_expanded = []\n    \n    # move image by moving_vectors and add to X_train_expanded\n    for i in range(X_train.shape[0]):\n        try:\n            for dx, dy in random.sample(moving_vectors, n_shifts):\n                shifted_image = shift_instance(X_train[i], dx=dx, dy=dy, new=0, im_size=im_size)\n                X_train_expanded.append(shifted_image)\n                y_train_expanded.append(y_train[i])\n        except ValueError:\n            print(\"Using maximal possible shifts. This might be smaller thatn n_shifts.\")\n            for dx, dy in moving_vectors:\n                shifted_image = shift_instance(X_train[i], dx=dx, dy=dy, new=0, im_size=im_size)\n                X_train_expanded.append(shifted_image)\n                y_train_expanded.append(y_train[i])\n\n    X_train_expanded = np.array(X_train_expanded, dtype=np.uint8)\n    y_train_expanded = np.array(y_train_expanded, dtype=np.uint8)\n    \n    # shuffle the new shifted images to avoid classifiers see the same number for too many times\n    if do_shuffle:\n        X_train_expanded, y_train_expanded = shuffle(X_train_expanded, y_train_expanded, random_state=random_state)\n    return X_train_expanded, y_train_expanded\n\ndef rand_rotate_instance(instance, angle=10, new=0, im_size=(28,28)):\n    return rotate(instance.reshape(im_size), angle=angle, cval=new, reshape=False).reshape(im_size[0]*im_size[1])\n\ndef rand_rotate_training(X_train, y_train, angle_range=(5,10), im_size=(28,28),\n                        sign=True, do_shuffle=True, random_state=None):\n    \n    if not random_state is None:\n        np.random.seed(random_state)\n    \n    X_rot = X_train.copy()\n    for i in range(X_train.shape[0]): # we need this loop to not always get the same rotation\n        angle = np.random.randint(angle_range[0],angle_range[1])\n        X_rot[i] = rand_rotate_instance(X_train[i],angle=angle,im_size=im_size)\n    \n    if sign:\n        X_rot_sign = X_train.copy()\n        for i in range(X_train.shape[0]): # we need this loop to not always get the same rotation\n            angle = np.random.randint(angle_range[0],angle_range[1])\n            X_rot_sign[i] = rand_rotate_instance(X_train[i],angle=-angle,im_size=im_size)\n        \n        # shuffle the new shifted images to avoid classifiers see the same number for too many times\n        if do_shuffle:\n            X_train_expanded, y_train_expanded = shuffle(np.concatenate((X_rot, X_rot_sign)),\n                                                         np.concatenate((y_train, y_train)),\n                                                         random_state=random_state)\n        return X_train_expanded, y_train_expanded    \n    else:\n        return X_rot, y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally, I would like to test the performance on the augmented set, but it would take too long time. *Because I tested this...*\n\nNext, we are anyway writing a preprocessor class with some of our steps done so far.*That was my aim, but there are problems implementing data augmentation as one has to take care of passing a bigger target array. As far as I see this, sklearn classes are only meant to pass the training data in the transform method.*"},{"metadata":{},"cell_type":"markdown","source":"## Fine tuning of our classifiers\n\nFirst, we will transform our preprocessing steps into a custom transformer. Then we will do hyperparameter tuning on (hopefully) augmented, polluted, resized, cropped, stratified training data. Hopefully, the hyperparameters will be good enough when we will fit our models overnight on the augmented or augmented polluted training instances. So lets start with our custom transformer class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, resize=True, pollute=True, crop=True, stratify=True, n_poll_pic=1, random_state=None): # pass only whether to apply some transformations and random state\n        self.resize = resize\n        self.crop = crop\n        self.stratify = stratify\n        self.pollute = pollute\n        self.n_poll_pic = n_poll_pic\n        self.random_state = random_state\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X): \n        if self.pollute:\n            X = pollute_training(X,n_poll_pic = self.n_poll_pic, random_state=self.random_state)\n        if self.resize:\n            if X.shape[1] != 14*14: # only resize if pictures are not already resized to (14,14)\n                X = resize_train(X)\n        if self.crop:\n            X = crop_data(X, tr_im_size=[14,14], crop_left=1, crop_right=1, crop_top=1, crop_bottom=1)\n        if self.stratify:\n            stratify_data(X, threshold=0)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_training(X_train, y_train,dx_max=2,dy_max=2,n_shifts = 4,angle_range=(5,10),im_size=(28,28),\n                     only_axes=False, do_shuffle=True, sign=True, random_state=None):\n    X_shift, y_shift = rand_shift_training(X_train, y_train, dx_max=dx_max, dy_max=dy_max, n_shifts = n_shifts, \n                        im_size=im_size, random_state=random_state, only_axes=only_axes, do_shuffle=do_shuffle)\n    X_rot, y_rot = rand_rotate_training(X_train, y_train, angle_range=angle_range, im_size=im_size,\n                        sign=sign, do_shuffle=do_shuffle, random_state=random_state)\n    X_train = np.concatenate((X_train,X_shift, X_rot))\n    y_train = np.concatenate((y_train, y_shift, y_rot))\n    return X_train, y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The problem has already be explained: I would have liked to put augment_training into the class, but passing X and the target y into the transform method does not seem to fit the design of scikit-learn. Note that the class could be much more complete by passing the arguments of the functions when initializin, but I was too lazy to enable the class to do more than the default values of the functions.*\n\n*What I did try first was to augment the training data with 4 shifts and two rotations of each instance, but running into memory problems. I restricted to 2 shifts, and polluted them, then resizing, then cropping and stratifying. But this took way too long because resizing the pictures takes time. So I was forced to first resize the pictures and then augment them (with maximal lenght of shifts 1).*\n\n*Before augmenting the data, I wanted to look how polluting, cropping and stratifying affects the classifiers. This was way too greedy, as it took roughly 40 minutes:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"if False: # WARNING:took 2457 seconds on my machine\n    knn = KNeighborsClassifier()\n    sgd = make_pipeline(StandardScaler(),SGDClassifier(random_state=43)) # SGD needs all features to have a similar scale (which we have), so we do not try scaling yet\n    x_trees = ExtraTreesClassifier(random_state=43)\n    svc_rbf = make_pipeline(StandardScaler(),SVC(kernel='rbf', random_state=43)) #  kernel='rbf' is the default in sklearn\n    svc_nyst_rbf = make_pipeline(StandardScaler(),Nystroem(kernel='rbf', random_state=43),LinearSVC())\n    grad_boost = GradientBoostingClassifier(random_state=43)\n\n    # now resize True for predicting\n    im_transf = ImageTransformer(resize=True, pollute=True, crop=True, stratify=True,random_state=43)\n    im_transf2 = ImageTransformer(resize=False, pollute=False, crop=True, stratify=True,random_state=43)\n\n    classifiers = [knn, sgd, x_trees, svc_rbf, svc_nyst_rbf]\n    params = [{'crop': [True,False], 'stratify': [True,False]}]\n\n    X_val_res = resize_train(X_val) # so to not always resize again in the loop; we do not want pollute validating set\n\n    start = time.time()\n    for pol in [True, False]:\n        im_transf.set_params(**{'resize': True, 'pollute': pol, 'crop': False, 'stratify': False,'random_state': 43})\n        X = im_transf.transform(X_train)\n        for dic in list(ParameterGrid(params)):\n            im_transf2.set_params(**dic)\n            print(\"Pollution is set to\", pol)\n            print(\"Now apply ImageTransformer with\", im_transf2.get_params())\n            for clf in classifiers: # reminder: classifiers = [knn,sgd,x_trees,svc_rbf,lin_svc]\n                print(clf.__class__.__name__)\n                fit_score_time(clf, im_transf2.transform(X), y_train, im_transf2.transform(X_val_res), y_val, prnt=True, oob=False)\n                print(\"**************************************************************\")\n\n    end = time.time()\n    print(\"Total time needed:\", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*As you easily see, the output is a pain to evaluate.*\n\n**Update** *Due to upload you don't see the inital output again, but its a long long list of the kind of cell 24, only longer and impractical to use*\n## Failure: Care for a lucid overview of your results\n*But if you do, you will see that the transformations applied nearly make no difference.*\n\n*I next tried to pollute the pictures more, setting n_poll_pic = 10, which results in the following:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"im_transf = ImageTransformer(resize=True, pollute=True, crop=True, stratify=False, n_poll_pic=10, random_state=43)\nplt.figure(figsize=(10,10))\nplot_digits(im_transf.transform(X_train[:100]), images_per_row=10, size=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The classifiers responded worse on these pictures. So I decided to just use resizing, because it was the only technique that gave a real boost in computing time with nearly the same score.*\n\n*Next, I started tuning the hyperparameters on the training set or on the first 10000 instances of the training set, hoping they would generalize well to the better training data. Like this:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# thats the training data we work with\nim_transf = ImageTransformer(resize=True, pollute=False, crop=False, stratify=False,random_state=43)\nX = im_transf.transform(X_train)\nX_val_res = im_transf.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_jobs=-1)\nclf_params = [{'n_neighbors': [2,4,8], 'weights': ['uniform', 'distance']}]\n\nfor param in list(ParameterGrid(clf_params)): # reminder: classifiers = [knn,sgd,x_trees,svc_rbf,lin_svc]\n    print(clf.__class__.__name__)\n    clf.set_params(**param)\n    print(param)\n    fit_score_time(clf, X[:10000], y_train[:10000], X_val_res, y_val, prnt=True, oob=False)\n    print(\"**************************************************************\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Below is the moment I realized something strange was going on when using the LabelBinarizer on the targets with ExtraTreeClassifer or RandomForestClassifier: The oob_score differs much from the validation score:*"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clf = ExtraTreesClassifier(random_state=43, n_jobs=-1, oob_score = True, bootstrap=True)\nclf_params = [{'max_depth': [3,9,14], 'min_samples_split': [50, 100,500]}]\n\nfor param in list(ParameterGrid(clf_params)): # reminder: classifiers = [knn,sgd,x_trees,svc_rbf,lin_svc]\n    print(clf.__class__.__name__)\n    clf.set_params(**param)\n    print(param)\n    fit_score_time(clf, X, y_train_label, X_val_res, y_val_label, prnt=True, oob=False)\n    print(\"oob score is:\", clf.oob_score_)\n    print(\"**************************************************************\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# are the validation labels correct?\nprint(y_val[:20])\nprint(y_val_label[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*For some reason, working with the labels results in poor score on the validation set. I haven't rack my brain on this issue yet and welcome any explanation.*\n\n*All in all, I ended with the configuration below:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# so lets put them together into a voting team\nsvc_nyst = Pipeline([('std_scaler',StandardScaler()),\n                ('nystroem',Nystroem(kernel='rbf', gamma=0.0034, random_state=43)),\n                ('lin_svc',LinearSVC(C=5, dual=False, random_state=43))])\nsvc_rbf = Pipeline([('std_scaler',StandardScaler()),('rbf',SVC(kernel='rbf', C=5, gamma=0.005, random_state=43))])\nrnd_for = RandomForestClassifier(random_state=43, n_jobs=-1)\nx_trees = ExtraTreesClassifier(random_state=43, n_jobs=-1)\nknn = KNeighborsClassifier(n_neighbors=4, weights='distance')\nsgd = make_pipeline(StandardScaler(),SGDClassifier(alpha= 0.0005, loss='hinge', penalty='l2', n_jobs=-1,\n                                                        random_state=43))\n\nvote_fast = VotingClassifier([('nystroem', svc_nyst), ('sgd',sgd)])\nvote_slow = VotingClassifier([('rbf', svc_rbf), ('x_trees',x_trees), ('rnd_for',rnd_for), ('knn',knn)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*I trained the fast classifiers on the augmented training set*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_aug, y_aug = augment_training(resize_train(X_train),y_train,\n                                dx_max=2,dy_max=2, n_shifts=4, im_size=(14,14),random_state=43)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nvote_fast.fit(X_aug,y_aug)\nend = time.time()\nprint(\"Time fitting the fast voters:\", end-start) # 132 seconds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*and the slow classifiers on the resized original training set*"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nvote_slow.fit(X_res,y_train)\nend = time.time()\nprint(\"Time fitting the slow voters:\", end-start) # 150 seconds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Here is the validation and test score for the slow voters*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_res = resize_train(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score on the validation set:\",accuracy_score(y_val, vote_slow.predict(X_val_res)))\nprint(\"Score on the test set:\",accuracy_score(y_test, vote_slow.predict(X_test_res)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for estimator in vote_slow.estimators_: # svc_rbf, extra_trees, random_forest, knn\n    print(\"Score of\", estimator.__class__.__name__, \"on the validation set:\", estimator.score(X_val_res, y_val))\n    print(\"Score of\", estimator.__class__.__name__, \"on the test set:\", estimator.score(X_test_res, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*And here for the fast classifiers:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score on the validation set:\",accuracy_score(y_val, vote_fast.predict(X_val_res)))\nprint(\"Score on the test set:\",accuracy_score(y_test, vote_fast.predict(X_test_res)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for estimator in vote_fast.estimators_: # nymstroem, sgd\n    print(\"Score of\", estimator.__class__.__name__, \"on the validation set:\", estimator.score(X_val_res, y_val))\n    print(\"Score of\", estimator.__class__.__name__, \"on the test set:\", estimator.score(X_test_res, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*That's the moment where I had to laugh. Funnily, the quick classifiers got much worse on the augmented dataset and I don't know why. In fact, I did some \"quick\" parameter search, but they tend to stay worse than the training on the original resized training set. Any idea?*\n\n*Of course, one could go on analyzing the misclassified pictures or test other configurations, see how the slow classifiers behave on an augmented set and so on. But for me, this should suffice for a while working on mnist. Just predict the test data and hopefully see mnist again with neural networks.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_eval = df_test.to_numpy()\nX_eval_res = resize_train(X_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this takes long because knn needs some time to predict 28000 instances\n#y_eval = vote_slow.predict(X_eval_res) t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = pd.DataFrame(y_eval, columns=['Label'])\n#df.index = np.arange(1, len(df)+1)\n#df.to_csv(../output/submission.csv, index_label='ImageId') # scored 0.97342","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":4}